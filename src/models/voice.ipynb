{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(waveform, sample_rate):\n",
    "    mfcc_transform = torchaudio.transforms.MFCC(\n",
    "        sample_rate=sample_rate,\n",
    "        n_mfcc=40,\n",
    "        melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 40}\n",
    "    )\n",
    "    return mfcc_transform(waveform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * 20 * 20, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_one(input1)\n",
    "        output2 = self.forward_one(input2)\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = nn.functional.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(file_path, sample_rate=16000):\n",
    "    waveform, sr = torchaudio.load(file_path)\n",
    "    if sr != sample_rate:\n",
    "        waveform = torchaudio.functional.resample(waveform, sr, sample_rate)\n",
    "    return extract_mfcc(waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (input1, input2, label) in enumerate(train_loader):\n",
    "            output1, output2 = model(input1, input2)\n",
    "            loss = criterion(output1, output2, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate(model, user_voice, test_voice, threshold=0.5):\n",
    "    with torch.no_grad():\n",
    "        output1, output2 = model(user_voice.unsqueeze(0), test_voice.unsqueeze(0))\n",
    "        distance = nn.functional.pairwise_distance(output1, output2)\n",
    "        return distance.item() < threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    your_voice_samples = [load_and_preprocess_data(f\"your_voice_{i}.wav\") for i in range(5)]\n",
    "    random_voices = [load_and_preprocess_data(f\"random_voice_{i}.wav\") for i in range(100)]\n",
    "\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    for i in range(len(your_voice_samples)):\n",
    "        for j in range(i+1, len(your_voice_samples)):\n",
    "            pairs.append((your_voice_samples[i], your_voice_samples[j]))\n",
    "            labels.append(0) \n",
    "        \n",
    "        for random_voice in random_voices:\n",
    "            pairs.append((your_voice_samples[i], random_voice))\n",
    "            labels.append(1) \n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(pairs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(list(zip(X_train, y_train)), batch_size=32, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(list(zip(X_test, y_test)), batch_size=32, shuffle=False)\n",
    "\n",
    "    model = SiameseNetwork()\n",
    "    criterion = ContrastiveLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_model(model, train_loader, criterion, optimizer)\n",
    "\n",
    "    test_voice = load_and_preprocess_data(\"test_voice.wav\")\n",
    "    for i, user_voice in enumerate(your_voice_samples):\n",
    "        is_authenticated = authenticate(model, user_voice, test_voice)\n",
    "        print(f\"Authentication result for sample {i+1}: {'Authenticated' if is_authenticated else 'Not Authenticated'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
