{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: switch all this data preprocessing to the yet to implement PySpark module.\n",
    "class AudioPreprocessor:\n",
    "    def __init__(self, sample_rate=16000, duration=3, n_mels=40, n_fft=1024, hop_length=512):\n",
    "        self.sample_rate = sample_rate\n",
    "        self.duration = duration\n",
    "        self.n_samples = sample_rate * duration\n",
    "        self.n_mels = n_mels\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "\n",
    "    def convert_audio_to_wav(self, input_path):\n",
    "        try:\n",
    "            temp_dir = tempfile.mkdtemp()\n",
    "            temp_path = os.path.join(temp_dir, 'temp.wav')\n",
    "            \n",
    "            audio = AudioSegment.from_file(input_path)\n",
    "            \n",
    "            if audio.channels > 1:\n",
    "                audio = audio.set_channels(1)\n",
    "            \n",
    "            audio = audio.set_frame_rate(self.sample_rate)\n",
    "            \n",
    "            audio.export(temp_path, format='wav')\n",
    "            \n",
    "            return temp_path\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting audio file {input_path}: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def load_and_preprocess_file(self, file_path):\n",
    "        try:\n",
    "            file_path = str(Path(file_path).resolve())\n",
    "            \n",
    "            print(f\"Processing file...\")\n",
    "            \n",
    "            temp_wav_path = self.convert_audio_to_wav(file_path)\n",
    "            \n",
    "            try:\n",
    "                waveform, sr = sf.read(temp_wav_path)\n",
    "                print(f\"Successfully loaded audio file. Sample rate: {sr}, Shape: {waveform.shape}\")\n",
    "                \n",
    "                os.remove(temp_wav_path)\n",
    "                os.rmdir(os.path.dirname(temp_wav_path))\n",
    "            \n",
    "                if sr != self.sample_rate:\n",
    "                    waveform = librosa.resample(waveform, orig_sr=sr, target_sr=self.sample_rate)\n",
    "                \n",
    "                if len(waveform) < self.n_samples:\n",
    "                    waveform = np.pad(waveform, (0, self.n_samples - len(waveform)))\n",
    "                else:\n",
    "                    waveform = waveform[:self.n_samples]\n",
    "                \n",
    "                mel_spec = librosa.feature.melspectrogram(\n",
    "                    y=waveform, \n",
    "                    sr=self.sample_rate,\n",
    "                    n_mels=self.n_mels,\n",
    "                    n_fft=self.n_fft,\n",
    "                    hop_length=self.hop_length\n",
    "                )\n",
    "                \n",
    "                mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "                \n",
    "                mel_spec_db = (mel_spec_db - mel_spec_db.mean()) / (mel_spec_db.std() + 1e-9)\n",
    "                \n",
    "                mel_spec_tensor = torch.FloatTensor(mel_spec_db).unsqueeze(0)\n",
    "                \n",
    "                return mel_spec_tensor\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing converted WAV file: {str(e)}\")\n",
    "\n",
    "                if os.path.exists(temp_wav_path):\n",
    "                    os.remove(temp_wav_path)\n",
    "                    os.rmdir(os.path.dirname(temp_wav_path))\n",
    "                raise\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: switch this CNN to a siamese network\n",
    "class VoiceAuthNetwork(nn.Module):\n",
    "    def __init__(self, input_shape=(40, 94), threshold=0.5):\n",
    "        super(VoiceAuthNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        conv_output_h = input_shape[0] // 2\n",
    "        conv_output_w = input_shape[1] // 2\n",
    "        self.conv_output_size = 128 * conv_output_h * conv_output_w\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.conv_output_size, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        self.threshold = threshold\n",
    "        self.register_buffer('center', torch.zeros(64))\n",
    "        self.register_buffer('n_samples', torch.tensor(0))\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def compute_center(self, owner_samples):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            embeddings = []\n",
    "            for sample in owner_samples:\n",
    "                embedding = self.forward_one(sample.unsqueeze(0))\n",
    "                embeddings.append(embedding)\n",
    "            \n",
    "            embeddings = torch.cat(embeddings, dim=0)\n",
    "            self.center = embeddings.mean(dim=0)\n",
    "            self.n_samples = torch.tensor(len(owner_samples))\n",
    "\n",
    "    def authenticate(self, audio_sample):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            embedding = self.forward_one(audio_sample.unsqueeze(0))\n",
    "            distance = F.pairwise_distance(embedding, self.center.unsqueeze(0))\n",
    "            return distance.item() < self.threshold\n",
    "\n",
    "    def update_threshold(self, owner_samples, percentile=95):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            distances = []\n",
    "            for sample in owner_samples:\n",
    "                embedding = self.forward_one(sample.unsqueeze(0))\n",
    "                distance = F.pairwise_distance(embedding, self.center.unsqueeze(0))\n",
    "                distances.append(distance.item())\n",
    "            \n",
    "            self.threshold = np.percentile(distances, percentile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_folder='data/owner'):\n",
    "    preprocessor = AudioPreprocessor()\n",
    "    processed_samples = []\n",
    "    \n",
    "    try:\n",
    "        data_folder = str(Path(data_folder).resolve())\n",
    "        print(f\"Looking for audio files in: {data_folder}\")\n",
    "        \n",
    "        if not os.path.isdir(data_folder):\n",
    "            raise ValueError(f\"Directory not found: {data_folder}\")\n",
    "        \n",
    "        audio_extensions = ('.wav', '.mp3', '.m4a', '.flac', '.ogg')\n",
    "        audio_files = [f for f in os.listdir(data_folder) \n",
    "                       if f.lower().endswith(audio_extensions)]\n",
    "        \n",
    "        if not audio_files:\n",
    "            raise ValueError(f\"No audio files found in {data_folder}\")\n",
    "        \n",
    "        print(f\"Found {len(audio_files)} audio files\")\n",
    "        \n",
    "        for audio_file in audio_files:\n",
    "            file_path = os.path.join(data_folder, audio_file)\n",
    "            try:\n",
    "                processed_sample = preprocessor.load_and_preprocess_file(file_path)\n",
    "                processed_samples.append(processed_sample)\n",
    "                print(f\"Successfully processed: {audio_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to process {audio_file}: {str(e)}\")\n",
    "        \n",
    "        if not processed_samples:\n",
    "            raise ValueError(\"No audio files were successfully processed\")\n",
    "        \n",
    "        processed_samples = torch.stack(processed_samples)\n",
    "        \n",
    "        print(f\"Final tensor shape: {processed_samples.shape}\")\n",
    "        return processed_samples\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing data: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_audio_file(file_path):\n",
    "    try:\n",
    "        audio = AudioSegment.from_file(file_path)\n",
    "        print(f\"File: {file_path}\")\n",
    "        print(f\"Channels: {audio.channels}\")\n",
    "        print(f\"Sample width: {audio.sample_width}\")\n",
    "        print(f\"Frame rate: {audio.frame_rate}\")\n",
    "        print(f\"Frame count: {len(audio)}\")\n",
    "        print(f\"Duration: {len(audio) / 1000.0} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error diagnosing file {file_path}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_voice_auth(model, owner_samples, epochs=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for i, sample in enumerate(owner_samples):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            embedding = model.forward_one(sample.unsqueeze(0))\n",
    "            \n",
    "            if model.n_samples > 0:\n",
    "                loss = F.mse_loss(embedding, model.center.unsqueeze(0))\n",
    "            else:\n",
    "                loss = torch.tensor(0.0, requires_grad=True)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        model.compute_center(owner_samples)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(owner_samples):.4f}\")\n",
    "    \n",
    "    model.update_threshold(owner_samples)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticate_audio(model, audio_file):\n",
    "    preprocessor = AudioPreprocessor()\n",
    "    try:\n",
    "        audio_tensor = preprocessor.load_and_preprocess_file(audio_file)\n",
    "        return model.authenticate(audio_tensor)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during authentication: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diagnosing audio files...\n",
      "File: data/owner/1_owner.wav\n",
      "Channels: 1\n",
      "Sample width: 4\n",
      "Frame rate: 48000\n",
      "Frame count: 8400\n",
      "Duration: 8.4 seconds\n",
      "File: data/owner/0_owner.wav\n",
      "Channels: 1\n",
      "Sample width: 4\n",
      "Frame rate: 48000\n",
      "Frame count: 4320\n",
      "Duration: 4.32 seconds\n",
      "File: data/owner/3_owner.wav\n",
      "Channels: 1\n",
      "Sample width: 4\n",
      "Frame rate: 48000\n",
      "Frame count: 4500\n",
      "Duration: 4.5 seconds\n",
      "File: data/owner/2_owner.wav\n",
      "Channels: 1\n",
      "Sample width: 4\n",
      "Frame rate: 48000\n",
      "Frame count: 5100\n",
      "Duration: 5.1 seconds\n",
      "\n",
      "Starting data preparation...\n",
      "Looking for audio files in: /Users/rishiviswanathan/Desktop/voice-auth/src/models/data/owner\n",
      "Found 4 audio files\n",
      "Processing file...\n",
      "Successfully loaded audio file. Sample rate: 16000, Shape: (134400,)\n",
      "Successfully processed: 1_owner.wav\n",
      "Processing file...\n",
      "Successfully loaded audio file. Sample rate: 16000, Shape: (69120,)\n",
      "Successfully processed: 0_owner.wav\n",
      "Processing file...\n",
      "Successfully loaded audio file. Sample rate: 16000, Shape: (72000,)\n",
      "Successfully processed: 3_owner.wav\n",
      "Processing file...\n",
      "Successfully loaded audio file. Sample rate: 16000, Shape: (81600,)\n",
      "Successfully processed: 2_owner.wav\n",
      "Final tensor shape: torch.Size([4, 1, 40, 94])\n",
      "Input shape determined as: (40, 94)\n",
      "Creating and training model...\n",
      "Epoch 1/10, Loss: 0.0000\n",
      "Epoch 2/10, Loss: 5.0468\n",
      "Epoch 3/10, Loss: 0.0478\n",
      "Epoch 4/10, Loss: 0.0021\n",
      "Epoch 5/10, Loss: 0.0002\n",
      "Epoch 6/10, Loss: 0.0000\n",
      "Epoch 7/10, Loss: 0.0000\n",
      "Epoch 8/10, Loss: 0.0000\n",
      "Epoch 9/10, Loss: 0.0000\n",
      "Epoch 10/10, Loss: 0.0000\n",
      "Saving model...\n",
      "Model saved to: voice_auth_model.pth\n",
      "Testing authentication with file: data/owner/0_owner.wav\n",
      "Processing file...\n",
      "Successfully loaded audio file. Sample rate: 16000, Shape: (69120,)\n",
      "Authentication result: Rejected\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        data_folder = 'data/owner' \n",
    "        \n",
    "        print(\"Diagnosing audio files...\")\n",
    "        for audio_file in os.listdir(data_folder):\n",
    "            if audio_file.lower().endswith(('.wav', '.mp3', '.m4a', '.flac', '.ogg')):\n",
    "                diagnose_audio_file(os.path.join(data_folder, audio_file))\n",
    "        \n",
    "        print(\"\\nStarting data preparation...\")\n",
    "        owner_samples = prepare_data(data_folder=data_folder)\n",
    "        \n",
    "        input_shape = tuple(owner_samples[0].shape[1:])\n",
    "        print(f\"Input shape determined as: {input_shape}\")\n",
    "        \n",
    "        print(\"Creating and training model...\")\n",
    "        model = VoiceAuthNetwork(input_shape=input_shape)\n",
    "        model = train_voice_auth(model, owner_samples)\n",
    "        \n",
    "        print(\"Saving model...\")\n",
    "        save_path = 'voice_auth_model.pth'\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'input_shape': input_shape,\n",
    "            'threshold': model.threshold,\n",
    "            'center': model.center\n",
    "        }, save_path)\n",
    "        print(f\"Model saved to: {save_path}\")\n",
    "        \n",
    "        test_file = os.path.join(data_folder, '0_owner.wav') \n",
    "        if os.path.exists(test_file):\n",
    "            print(f\"Testing authentication with file: {test_file}\")\n",
    "            is_authenticated = authenticate_audio(model, test_file)\n",
    "            print(f\"Authentication result: {'Accepted' if is_authenticated else 'Rejected'}\")\n",
    "        else:\n",
    "            print(f\"Test file not found: {test_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
